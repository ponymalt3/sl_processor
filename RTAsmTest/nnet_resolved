



def wm8731_base 0xF00000;
def xorshift_base 0xF00100;

function xorshift_rand_0_to_1()
  a1=xorshift_base;
  return [a1];
end

function xorshift_rand_minus_1_to_1()
  a1=xorshift_base+1;
  return [a1];
end

function xorshift_rand_full()
  a1=xorshift_base+2;
  return [a1];
end

function xorshift_seed(seed)
  a1=xorshift_base+3;
  [a1]=seed;
end

def pi 3.141592654;
    
    function mod(a,b)
      div=a/b;
      return a-(int(div)*b);
    end

    function sqrt(x)
      s=shft(1.0,log2(x));
      s=2*s/(s*s+x);

      ## use 1/sqrt(x)
      loop(6)
        s=0.5*s*(3-x*s*s);
      end

      return s*x;
    end

    function sin(x)
      x=mod(x,2*pi);
      
      if(x > pi)
        x=-x+pi;
      end
      
      x2=x*x;

      ## taylor approximation
      t=x;
      x=x*x2;
      s=t-x*(1/6);
      x=x*x2;
      s=s+x*(1/120);
      x=x*x2;
      s=s-x*(1/5040);
      x=x*x2;
      s=s+x*(1/362880);

      return s;
    end

    function cos(x)
      return sin(x+0.5*pi);
    end

    function ln(x)
      return log2(x)*0.693147181;
    end

    function exp(x)
    
      x=x*1.442695041;
      ix=int(x);

      fx=x-ix;

      ## taylor approximation of 2^fraction
      t=0.693147181*fx;
      
      x=t;
      s=1+x;
      x=x*t;
      s=s+x*(1/2);
      x=x*t;
      s=s+x*(1/6);
      x=x*t;
      s=s+x*(1/24);
      
      return s*shft(1,ix);
      
    end
    
    function boxMuellerTransform(u1,u2,outAddr)
      a1=outAddr;
      t=sqrt(-2*ln(u1));
      t2=2*pi*u2;
      [a1++]=t*cos(t2);
      [a1++]=t*sin(t2);
    end

    function min(a,b)
      if(a < b)
        return a;
      else
        return b;
      end
    end

    function max(a,b)
      if(a < b)
        return b;
      else
        return a;
      end
    end

function rand()
  return xorshift_rand_0_to_1();
end

function createMem(addr,size)
  if(size <= 2)
    return 0;
  end

  a1=addr;
  [a1++]=size;
  [a1]=0; ## next ptr

  return addr;
end
  
function allocateMem(size,pool)

  if(size < 2)
    size=2;
  end
  
  head=pool;

  node=0;
  nodeSize=99999999;

  a1=head;
  while(head != 0)
    s=[a1++];
    
    if(s >= (size+3) and s < nodeSize) ## found possible block
      nodeSize=s;
      node=head;
    end

    head=[a1];
    a1=head;
  end

  if(node == 0)
    return 0;
  end

  a1=node;
  [a1]=[a1]-size-1;
  mem=node+[a1];
  a1=mem;
  [a1]=size;
  return mem+1;
end

function freeMem(mem)
  a1=mem-1;
end

function leakyReLu(sum)
  if(sum > 0)
    return sum;
  end

  return 0.01*sum;
end

function leakyReLuDif(sum)
  if(sum > 0)
    return 1;
  end

  return 0.01;
end

function abs(x)
  if(x < 0)
    return -x;
  end

  return x;
end


##input vector must be in local mem
## number of weights = number of inputs + 1 (bias)
function evaluteNodes(numNodes,weightMatrix,inputVec_local,numInputs,resultVec_local,sumVec)

  numWeights=numInputs+1;#bias included in weights

  blockSize=min(numWeights,128);##optimize block size for cache

  sums=sumVec;

  ## init sums with bias
  loop(numNodes)
    a1=weightMatrix+i*numWeights;
    bias=[a1];
    a1=sums+i;
    [a1]=bias;
  end
  
  numBlocks=int((numInputs+blockSize-1)/blockSize);
  loop(numBlocks)
    loop(numNodes)
      sum=0;
      a1=weightMatrix+numWeights*j+blockSize*i+1;
      a0=inputVec_local+blockSize*i;
      loop(min(blockSize,numInputs-blockSize*i))
        sum=sum+[a0++]*[a1++];
      end
      a1=sums+j;
      [a1]=[a1]+sum;
    end
  end

  a0=resultVec_local;
  a1=sums;
  loop(numNodes)
    [a0++]=leakyReLu([a1++]);
  end

  return sums;
end


function hadamard(vecA_local,vecB,size)
  a0=vecA_local;
  a1=vecB;
  loop(size)
    [a0++]=[a0]*[a1++];
  end
end

## cross entropy y*ln(a)+(1−y)*ln(1−a)

#function difOutput(y_local,output,size)
#  #(y1/a-(1-y)/(1-a))
#  a0=y_local;
#  a1=output;
#  loop(size)
#    [a0++]=[a0]/[a1]-(1-[a0])/(1-[a1++]);
#  end
#end

## mse sum((y_local-output)*(y_local-output))

function difOutput(y_local,output,size)
  #overwritten
  # y_local

  #2*y-2*a
  a0=y_local;
  a1=output;
  loop(size)
    [a0++]=([a0]-[a1++])*-2;
  end
end
  

function difLayer(vecDif_local,vecSum,size,vecInput_local,numInputs,matWeights,nextDifVec,learningRate)
  
  #overwritten
  # vecDif_local
  # matWeights
  # nextDifVec

  a0=vecDif_local;
  a1=vecSum;
  loop(size)
    [a0++]=[a0]*leakyReLuDif([a1++]);
  end

  ##generate next dif vector
  nextDif=nextDifVec;## allocate(numInputs);

  a0=nextDif;
  loop(numInputs)
    [a0++]=0;
  end

  loop(size)
    a0=vecDif_local+i;##contains derivation
    dif=[a0];
    a0=nextDif;
    a1=matWeights+(numInputs+1)*i+1;
    loop(numInputs)
      [a0++]=[a0]+dif*[a1++];
    end

    #lerning rate
    dif=dif*learningRate;

    ## update weights
    a1=matWeights+(numInputs+1)*i;
    a0=vecInput_local;
    [a1++]=[a1]-dif; ## adjust bias
    loop(numInputs)
      [a1++]=[a1]-([a0++]*dif);
    end
  end

  return nextDif;
end

function nnetCreate(layerConfigArray_local,layerConfigSize,LOCAL_MEM,GLOBAL_MEM)

  ## layer struc
  ## size
  ## node outputs + sums
  ## weights (size * next layer size)

  ## nnet
  ## num layers
  ## layer struc * num layers-1
  ## dif buffer 1 (max(layer size))
  ## dif buffer 2 (max(layer size))
  ## num inputs
  ## num outputs

  nnet=allocateMem(3*(layerConfigSize-1)+5,LOCAL_MEM);
  a1=nnet;
  [a1++]=layerConfigSize-1;
  maxLayerSize=0;
  loop(layerConfigSize-1) ## first layer is 'input'

    a0=layerConfigArray_local+i;
    inputs=[a0++];
    layerSize=[a0];
    numWeights=(inputs+1)*layerSize; ## allocate also space for bias

    outputs=allocateMem(layerSize*2,LOCAL_MEM); ## outputs + sums
    weights=allocateMem(numWeights+1,GLOBAL_MEM); ## allocate one more to make always be divideable by 2

    a1=nnet+1+i*3;
    [a1++]=layerSize; ## set size
    maxLayerSize=max(maxLayerSize,layerSize);
    [a1++]=outputs;
    [a1++]=weights;

    ## init weights
    loop(int((numWeights+1)/2))
      boxMuellerTransform(rand(),rand(),weights+j*2);
    end
  end

  mem=allocateMem(maxLayerSize*2,LOCAL_MEM);
  a1=nnet+1+(layerConfigSize-1)*3;
  [a1++]=mem;
  [a1++]=mem+maxLayerSize;

  a0=layerConfigArray_local;
  [a1++]=[a0];
  a0=layerConfigArray_local+layerConfigSize-1;
  [a1++]=[a0];  

  return nnet;
end
  
function nnetEval(nnet,input_local,inputSize)
  
  inputData=input_local;

  a0=nnet;
  numLayers=[a0];
  loop(numLayers)

    a0=nnet+1+i*3;
    size=[a0++];
    outputs=[a0++];
    weights=[a0++];
    
    evaluteNodes(size,weights,inputData,inputSize,outputs,outputs+size);
    inputData=outputs;
    inputSize=size;
  end

  return inputData;
end

function nnetUpdate(nnet,inputs_local,outputs_local,y)
  
  inputData=inputs_local;

  a0=nnet;
  numLayers=[a0];
  a0=nnet+1+numLayers*3;
  curDif=[a0++];
  nextDif=[a0++];
  numInputs=[a0++];
  numOutputs=[a0++];

  a0=curDif;
  a1=y;
  loop(numOutputs)
    [a0++]=[a1++];
  end

  difOutput(curDif,outputs_local,numOutputs); ## diff error to outputs

  a0=nnet+1+(numLayers-1)*3;
  size=[a0++];
  outputs=[a0++];
  weights=[a0++];

  loop(numLayers)

    if(i == (numLayers-1))
      prevSize=numInputs;
      prevOutputs=inputs_local;
    else
      a0=nnet+1+(numLayers-2-i)*3;
      prevSize=[a0++];
      prevOutputs=[a0++];
      prevWeights=[a0++];
    end

    difLayer(curDif,outputs+size,size,prevOutputs,prevSize,weights,nextDif,0.1);

    t=nextDif;
    nextDif=curDif;
    curDif=t;
 
    size=prevSize;
    outputs=prevOutputs;
    weights=prevWeights;
  end
end
