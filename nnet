include(math)
include(mem)

function leakyReLu(sum)
  if(sum > 0)
    return sum;
  end

  return 0.01*sum;
end

function leakyReLuDif(sum)
  if(sum > 0)
    return 1;
  end

  return 0.01;
end

function abs(x)
  if(x < 0)
    return -x;
  end

  return x;
end


##input vector must be in local mem
## number of weights = number of inputs + 1 (bias)
function evaluteNodes(numNodes,weightMatrix,inputVec_local,numInputs,resultVec_local,sumVec)

  numWeights=numInputs+1;#bias included in weights

  blockSize=min(numWeights,128);##optimize block size for cache

  sums=sumVec;

  ## init sums with bias
  loop(numNodes)
    a1=weightMatrix+i*numWeights;
    bias=[a1];
    a1=sums+i;
    [a1]=bias;
  end
  
  numBlocks=int((numInputs+blockSize-1)/blockSize);
  loop(numBlocks)
    loop(numNodes)
      sum=0;
      a1=weightMatrix+numWeights*j+blockSize*i+1;
      a0=inputVec_local+blockSize*i;
      loop(min(blockSize,numInputs-blockSize*i))
        sum=sum+[a0++]*[a1++];
      end
      a1=sums+j;
      [a1]=[a1]+sum;
    end
  end

  a0=resultVec_local;
  a1=sums;
  loop(numNodes)
    [a0++]=leakyReLu([a1++]);
  end

  return sums;
end


function hadamard(vecA_local,vecB,size)
  a0=vecA_local;
  a1=vecB;
  loop(size)
    [a0++]=[a0]*[a1++];
  end
end

## cross entropy y*ln(a)+(1âˆ’y)*ln(1âˆ’a)

#function difOutput(y_local,output,size)
#  #(y1/a-(1-y)/(1-a))
#  a0=y_local;
#  a1=output;
#  loop(size)
#    [a0++]=[a0]/[a1]-(1-[a0])/(1-[a1++]);
#  end
#end

## mse sum((y_local-output)*(y_local-output))

function difOutput(y_local,output,size)
  #overwritten
  # y_local

  #2*y-2*a
  a0=y_local;
  a1=output;
  loop(size)
    [a0++]=([a0]-[a1++])*-2;
  end
end
  

function difLayer(vecDif_local,vecSum,size,vecInput_local,numInputs,matWeights,nextDifVec,learningRate)
  
  #overwritten
  # vecDif_local
  # matWeights
  # nextDifVec

  a0=vecDif_local;
  a1=vecSum;
  loop(size)
    [a0++]=[a0]*leakyReLuDif([a1++]);
  end

  ##generate next dif vector
  nextDif=nextDifVec;## allocate(numInputs);

  a0=nextDif;
  loop(numInputs)
    [a0++]=0;
  end

  loop(size)
    a0=vecDif_local+i;##contains derivation
    dif=[a0];
    a0=nextDif;
    a1=matWeights+(numInputs+1)*i+1;
    loop(numInputs)
      [a0++]=[a0]+dif*[a1++];
    end

    #lerning rate
    dif=dif*learningRate;

    ## update weights
    a1=matWeights+(numInputs+1)*i;
    a0=vecInput_local;
    [a1++]=[a1]-dif; ## adjust bias
    loop(numInputs)
      [a1++]=[a1]-([a0++]*dif);
    end
  end

  return nextDif;
end

function nnetCreate(layerConfigArray_local,layerConfigSize,LOCAL_MEM,GLOBAL_MEM)

  ## layer struc
  ## size
  ## node outputs + sums
  ## weights (size * next layer size)

  ## nnet
  ## num layers
  ## layer struc * num layers-1
  ## dif buffer 1 (max(layer size))
  ## dif buffer 2 (max(layer size))
  ## num inputs
  ## num outputs

  nnet=allocateMem(3*(layerConfigSize-1)+5,LOCAL_MEM);
  a1=nnet;
  [a1++]=layerConfigSize-1;
  maxLayerSize=0;
  loop(layerConfigSize-1) ## first layer is 'input'

    a0=layerConfigArray_local+i;
    inputs=[a0++];
    layerSize=[a0];
    numWeights=(inputs+1)*layerSize; ## allocate also space for bias

    outputs=allocateMem(layerSize*2,LOCAL_MEM); ## outputs + sums
    weights=allocateMem(numWeights+1,GLOBAL_MEM); ## allocate one more to make always be divideable by 2

    a1=nnet+1+i*3;
    [a1++]=layerSize; ## set size
    maxLayerSize=max(maxLayerSize,layerSize);
    [a1++]=outputs;
    [a1++]=weights;

    ## init weights
    loop(int((numWeights+1)/2))
      boxMuellerTransform(rand(),rand(),weights+j*2);
    end
  end

  mem=allocateMem(maxLayerSize*2,LOCAL_MEM);
  a1=nnet+1+(layerConfigSize-1)*3;
  [a1++]=mem;
  [a1++]=mem+maxLayerSize;

  a0=layerConfigArray_local;
  [a1++]=[a0];
  a0=layerConfigArray_local+layerConfigSize-1;
  [a1++]=[a0];  

  return nnet;
end
  
function nnetEval(nnet,input_local,inputSize)
  
  inputData=input_local;

  a0=nnet;
  numLayers=[a0];
  loop(numLayers)

    a0=nnet+1+i*3;
    size=[a0++];
    outputs=[a0++];
    weights=[a0++];
    
    evaluteNodes(size,weights,inputData,inputSize,outputs,outputs+size);
    inputData=outputs;
    inputSize=size;
  end

  return inputData;
end

function nnetUpdate(nnet,inputs_local,outputs_local,y)
  
  inputData=inputs_local;

  a0=nnet;
  numLayers=[a0];
  a0=nnet+1+numLayers*3;
  curDif=[a0++];
  nextDif=[a0++];
  numInputs=[a0++];
  numOutputs=[a0++];

  a0=curDif;
  a1=y;
  loop(numOutputs)
    [a0++]=[a1++];
  end

  difOutput(curDif,outputs_local,numOutputs); ## diff error to outputs

  a0=nnet+1+(numLayers-1)*3;
  size=[a0++];
  outputs=[a0++];
  weights=[a0++];

  loop(numLayers)

    if(i == (numLayers-1))
      prevSize=numInputs;
      prevOutputs=inputs_local;
    else
      a0=nnet+1+(numLayers-2-i)*3;
      prevSize=[a0++];
      prevOutputs=[a0++];
      prevWeights=[a0++];
    end

    difLayer(curDif,outputs+size,size,prevOutputs,prevSize,weights,nextDif,0.1);

    t=nextDif;
    nextDif=curDif;
    curDif=t;
 
    size=prevSize;
    outputs=prevOutputs;
    weights=prevWeights;
  end
end
